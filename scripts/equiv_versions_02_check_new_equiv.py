#!/usr/bin/env python3
"""
Compare the latest Feather summary generated by equiv_versions_01_pr_scanner.py with the
canonical data in results.json and produce a CSV of missing equivalent releases.
"""

import json
import os
from pathlib import Path
import pandas as pd
import requests

# ------------------- Configuration -------------------
# Base directory where the PR-scanner stores its output
OUTPUT_DIR = Path("pr_versions")
# Remote location of the canonical results file
RESULTS_JSON = "https://raw.githubusercontent.com/ministero-salute/it-fse-accreditati/main/RESULTS/results.json"
# Where to write the missing-equivalents summary
SUMMARY_DIR = Path("comparison")
# -----------------------------------------------------


def remote_json_get(url: str) -> dict:
    """Download a JSON document from *url* and return the parsed dict."""
    resp = requests.get(url, timeout=30)
    resp.raise_for_status()
    return resp.json()

def load_results_json(path: str) -> pd.DataFrame:
    """Load the remote results.json and normalise the key columns."""
    # fetch the remote JSON
    data = remote_json_get(path)

    # Expect a top-level key "results" containing a list
    records = data.get("results", [])
    if records == []:
        raise ValueError("results.json contains no 'results' entries")
    df = pd.DataFrame(records)

    # Normalise columns (some entries may use different naming)
    df["vendor_lc"] = df["vendor"].str.lower().str.strip()
    df["app_id_lc"] = df["application_id"].str.lower().str.strip()
    df["version_lc"] = df["version"].astype(str).str.lower().str.strip()

    # Evito colonne non necessarie che possono portare duplicati (es: pi√π tipi doc)
    df = df.drop(["vendor", "application_id", "version", "doc_type",
                  "service", "date", "gtw_version"], axis=1)\
                    .drop_duplicates(subset=["vendor_lc", "app_id_lc", "version_lc"], keep="last")


    # Ensure equiv_releases is a list; missing ‚Üí empty list
    def ensure_list(val):
        """Return a list; empty list for NaN, keep list/tuple/set as-is."""
        if isinstance(val, (list, tuple, set)):
            return list(val)
        if pd.isna(val):
            return []
        return [val]

    df["equiv_releases"] = df["equiv_releases"].apply(ensure_list)


    # Normalise to a set of lower-cased strings
    df["equiv_set"] = df["equiv_releases"].apply(
        lambda lst: {str(v).strip().lower() for v in lst}
    )
    return df

def get_latest_feather_path() -> Path:
    """Return the most-recent *_pr_versions_summary_*.feather* file."""
    # Search for all feather files under OUTPUT_DIR/*/_pr_versions_summary_*.feather
    feather_files = sorted(OUTPUT_DIR.rglob("*_pr_versions_summary_*.feather"))
    if not feather_files:
        raise FileNotFoundError("No Feather summary files found under pr_versions/")
    # The newest file is the last one after sorting (lexicographic works because the date is in the name)
    return feather_files[-1]

def load_feather_summary(feather_path: Path) -> pd.DataFrame:
    """Read the Feather file and normalise the key columns."""
    df = pd.read_feather(feather_path)

    # Ensure expected columns exist
    required = {"Vendor", "App ID", "Main Version", "Equivalent Versions"}
    missing = required - set(df.columns)
    if missing:
        raise KeyError(f"Feather summary is missing columns: {missing}")

    # Normalise case for matching
    df["vendor_lc"] = df["Vendor"].str.lower().str.strip()
    df["app_id_lc"] = df["App ID"].str.lower().str.strip()
    df["version_lc"] = df["Main Version"].str.lower().str.strip()

    # Split the comma-separated equivalents into a set (lower-cased)
    def split_eq(s):
        if pd.isna(s) or not s:
            return set()
        return {v.strip().lower() for v in s.split(",") if v.strip()}
    df["equiv_set"] = df["Equivalent Versions"].apply(split_eq)

    return df


def compare_and_collect(feather_df: pd.DataFrame, results_df: pd.DataFrame) -> pd.DataFrame:
    """Return a DataFrame with rows that have missing equivalents."""
    # Merge on the three normalised keys
    merged = pd.merge(
        feather_df,
        results_df,
        on=["vendor_lc", "app_id_lc", "version_lc"],
        how="inner",
        suffixes=("_feather", "_results")
    )

    # Determine missing equivalents for each merged row
    def missing_eq(row):
        # rimuove la versione principale dalla serie di equivalenti da confrontare
        main = row["Main Version"].lower().strip()
        missing = row["equiv_set_feather"] - row["equiv_set_results"]
        missing.discard(main)  # se presente, rimuove la versione principale dalle equivalenze
        if missing:
            return ", ".join(sorted(missing))
        return ""

    merged["missing_equivalents"] = merged.apply(missing_eq, axis=1)

    # Keep only rows where something is missing
    missing_df = merged[merged["missing_equivalents"] != ""].copy()

    if missing_df.empty:
        raise ValueError("No missing equivalent releases found.")

    # Mantengo le equivalenze correnti per riferimento
    missing_df["current_equivalents"] = missing_df["equiv_set_results"].apply(
        lambda s: ", ".join(sorted(s))
    )

    # Clean up columns for the final report
    report = pd.DataFrame(missing_df[[
        "Vendor", "App ID", "Main Version",
        "current_equivalents", "missing_equivalents"
    ]])
    return report

def main():
    # 1Ô∏è‚É£ Locate the latest Feather summary
    feather_path = get_latest_feather_path()
    print(f"üîé Using Feather file: {feather_path}")

    # 2Ô∏è‚É£ Load data
    feather_df = load_feather_summary(feather_path)
    results_df = load_results_json(RESULTS_JSON)

    # 3Ô∏è‚É£ Compare
    missing_report = compare_and_collect(feather_df, results_df)

    # 4Ô∏è‚É£ Write output
    if missing_report.empty:
        print("‚úÖ No missing equivalent releases found.")
        return

    # Controllo eventuali applicativi duplicati nel report, possibili in caso di pi√π PR aperte 
    # per lo stesso software dall'ultima esecuzione dello script
    # if not (missing_report[["Vendor", "App ID", "Main Version"]].value_counts() == 1).all():
    # ---------------------------------------------------------
    # Count how many missing versions each row contains
    missing_report["_missing_cnt"] = missing_report["missing_equivalents"]\
        .fillna("")\
        .apply(lambda x: len([v for v in x.split(",") if v.strip()]))

    # Keep, for each (Vendor, App ID, Main Version), the row with the highest count
    idx = missing_report.groupby(
        ["Vendor", "App ID", "Main Version"]
    )["_missing_cnt"].idxmax()

    missing_report = missing_report.loc[idx].drop(columns=["_missing_cnt"])

    assert (missing_report[["Vendor", "App ID", "Main Version"]].value_counts() == 1).all(), "Duplicate apps found!"
    # ---------------------------------------------------------

    SUMMARY_DIR.mkdir(parents=True, exist_ok=True)
    today = pd.Timestamp.now().strftime("%Y-%m-%d")
    out_csv = SUMMARY_DIR / f"missing_equivalents_{today}.csv"
    missing_report.to_csv(out_csv, index=False)
    out_xlsx = SUMMARY_DIR / f"missing_equivalents_{today}.xlsx"
    missing_report.to_excel(out_xlsx, index=False, engine="openpyxl")
    print(f"üìù Missing equivalents written to: {out_csv} and {out_xlsx}")

if __name__ == "__main__":
    main()